# -*- coding: utf-8 -*-
"""FlanT5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K6EaUWIq1SAVWnJNvp7AMJA9l_IsWp4t
"""

!pip install datasets transformers accelerate peft

from datasets import load_dataset, Dataset
import pandas as pd

# dair-ai/emotion - Text Classification

text_classification_data = load_dataset("dair-ai/emotion", split="test")
text_classification_data

text_classification_df = text_classification_data.to_pandas()
text_classification_df

# sadness (0), joy (1), love (2), anger (3), fear (4), surprise (5)

map_emotions = {0: "sadness",
                1: "joy",
                2: "love",
                3: "anger",
                4: "fear",
                5: "surprise"
                }
text_classification_df["target"] = text_classification_df["label"].apply(lambda x: map_emotions[x])
text_classification_df["source"] = text_classification_df["text"].apply(lambda x: "text-")

text_classification_df = text_classification_df[["source", "target"]]
text_classification_df

"""Question Answering"""

data = load_dataset("mlqa", "mlqa.hi.en", split="validation")

qa_df = data.to_pandas()
qa_df

qa_df["source"] = qa_df[["context", "question"]].apply(lambda x: "context-question-answering: context: " + x["context"] + " question: " + x["question"], axis=1)
qa_df["target"] = qa_df["answers"].apply(lambda x: x["text"][0])

qa_df = qa_df[["source", "target"]]
qa_df

"""Question Gen"""

data = load_dataset("squad_v2", split="validation")
qgen_df = data.to_pandas()
qgen_df = qgen_df.sample(500)
qgen_df

qgen_df["source"] = qgen_df["context"].apply(lambda x: "context-question-generation: question: " + x)

qgen_df["target"] = qgen_df["question"]

qgen_df[["source", "target"]]

"""Paraphrasing -> Translation"""

data = load_dataset("paws", "labeled_final", split="validation")
trans_df = data.to_pandas()
trans_df = trans_df.sample(1000)
trans_df

trans_df = trans_df[trans_df["label"]==1]
trans_df = trans_df[["sentence1", "sentence2"]]
trans_df

trans_df["source"] = trans_df["sentence1"].apply(lambda x: "paraphrase: " + x)
trans_df["target"] = trans_df["sentence2"]
trans_df

!pip install py7zr

"""Summarization"""

data = load_dataset("samsum", split="test")
sum_df = data.to_pandas()
sum_df = sum_df[:500]
sum_df

sum_df["source"] = sum_df["dialogue"].apply(lambda x: "conversation-summarization: " + x)
sum_df["target"] = sum_df["summary"]
sum_df = sum_df[["source", "target"]]
sum_df

"""NLI"""

data = load_dataset("paws", "labeled_final", split="test")
nli_df = data.to_pandas()
nli_df = nli_df.sample(500)

nli_df

nli_df["source"] = nli_df[["sentence1", "sentence2"]].apply(lambda x: "nli-hypothesis-check: sentence1: " + x["sentence1"] + " sentence2: " + x["sentence2"], axis=1)
nli_df

hypo_dit = {
    0: "contradiction",
    1: "entailment",
}

nli_df["target"] = nli_df["label"].apply(lambda x: hypo_dit[x])
nli_df = nli_df[["source", "target"]]
nli_df

"""Final Data"""

list_of_dfs = [text_classification_df, qa_df, qgen_df, trans_df, sum_df, nli_df]

final_df = pd.concat(list_of_dfs, axis=0)
final_df = final_df[["source", "target"]]
final_df

"""Fine-Tune

"""

import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from peft import LoraConfig, get_peft_model
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq
from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(final_df, test_size=0.1, random_state=0, shuffle=True)
train_data = Dataset.from_pandas(train_df)
test_data = Dataset.from_pandas(test_df)

model_id="google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(model_id)

def preprocess_function(sample,padding="max_length"):
    model_inputs = tokenizer(sample["source"], max_length=256, padding=padding, truncation=True)
    labels = tokenizer(sample["target"], max_length=128, padding=padding, truncation=True)
    if padding == "max_length":
        labels["input_ids"] = [
            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels["input_ids"]
        ]
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

train_tokenized_dataset = train_data.map(preprocess_function, batched=True, remove_columns=train_data.column_names)
test_tokenized_dataset = test_data.map(preprocess_function, batched=True, remove_columns=test_data.column_names)
print(f"Keys of tokenized dataset: {list(train_tokenized_dataset.features)}")

lora_config = LoraConfig(
 r=8,
 lora_alpha=16,
 lora_dropout=0.1,
 bias="none",
 task_type="SEQ_2_SEQ_LM",
 target_modules=["q", "v"]
)

model

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

label_pad_token_id = -100
data_collator = DataCollatorForSeq2Seq(
    tokenizer,
    model=model,
    label_pad_token_id=label_pad_token_id,
    pad_to_multiple_of=8
)

from huggingface_hub import notebook_login
notebook_login()

output_dir="flant5-large-aio"
training_args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=6,
    learning_rate=2e-4,
    num_train_epochs=1,
    logging_dir=f"{output_dir}/logs",
    logging_strategy="epoch",
    save_strategy="epoch",
    report_to="tensorboard",
    push_to_hub = True
)

model.config.use_cache = False

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_tokenized_dataset
)

trainer.train()

peft_save_model_id="flant5-large-aio"
trainer.model.save_pretrained(peft_save_model_id, push_to_hub=True)
tokenizer.save_pretrained(peft_save_model_id, push_to_hub=True)
trainer.model.base_model.save_pretrained(peft_save_model_id, push_to_hub=True)

